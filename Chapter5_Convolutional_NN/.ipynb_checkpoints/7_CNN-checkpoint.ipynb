{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52162076-da2fbe00-2711-11e9-8cc7-eca21edb9c8b.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution을 정의하기 전에 im2col이라는 함수 생성\n",
    "# im2col은 입력 데이터를 필터링 하기 좋게 전개하는 함수.\n",
    "# 3차원 입력 데이터에 im2col을 적용하면 2차원 행렬로 바뀝니다.\n",
    "\n",
    "#img2col\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52162205-9f2e8a00-2713-11e9-94e9-8984011f972a.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "# 실제로 사용\n",
    "x1 = np.random.rand(1,3,7,7) #(데이터 수, 채널 수 , 높이, 너비)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad = 0)\n",
    "print(col1.shape) # stride 1이므로 3 X 3 = 9, 5X5X3 = 75 \n",
    "x2 = np.random.rand(10,3,7,7)\n",
    "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
    "print(col2.shape) # stride 1, 데이터 수 10개   3X3X10 = 90 , 5X5X3 = 75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self,w,b,stride=1, pad= 0):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "        \n",
    "        #중간 데이터 (backward시 사용)\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_w = None\n",
    "        \n",
    "        # 가중치와 편향 매개변수의 기울기\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self,x):\n",
    "        FN, C, FH, FW = self.w.shape # FN:  필터개수, C: 채널수, FH: 필터높이. FW:필터너비\n",
    "        N, C, H, W = x.shape # N개의 데이터\n",
    "        out_h = int(1+(H+ 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1+(W+ 2*self.pad - FW) / self.stride) # im2col로 펴진 데이터를 reshape위해\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.w.reshape(FN, -1).T # 필터전개\n",
    "        out = np.dot(col,col_W) + self.b # (28X28, 6) = (784,6)\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0,3,1,2) # (N, H, W, C) -> (N, C, H, W)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.w.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1,FN)\n",
    "        \n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dw = np.dot(self.col.T, dout)\n",
    "        self.dw = self.dw.transpose(1,0).reshape(FN, C, FH, FW)\n",
    "        \n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.s.shape, FH, FW, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.randn으로 w,b,x 임의로 지정, batch_size = 1\n",
    "x = np.random.rand(1, 3, 32, 32)\n",
    "w = np.random.rand(6, 3, 5, 5)\n",
    "b = np.random.rand(784,6)\n",
    "\n",
    "conv = Convolution(w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 28, 28)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.forward(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52167071-c0fe3000-2758-11e9-829f-6b0c74d50b53.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이번에는 padding을 입혀보자\n",
    "x = np.random.rand(1,3,7,7)\n",
    "w = np.random.rand(6,3,3,3) # 6filter 3X3 filter\n",
    "b = np.random.rand(49,6)      \n",
    "conv = Convolution(w,b,pad=1)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 7, 7)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.forward(x).shape # 7x7 output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 다른것도 해보자.\n",
    "x2 = np.random.rand(1,3,7,7)\n",
    "w2 = np.random.rand(6,3,5,5) # 6filter 5X5 filter\n",
    "b2 = np.random.rand(49,6)      \n",
    "conv2 = Convolution(w2,b2,pad=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 7, 7)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.forward(x2).shape # 7x7 output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 다른것도 해보자.\n",
    "x3 = np.random.rand(1,3,7,7)\n",
    "w3 = np.random.rand(6,3,7,7) # 6filter 7X7 filter\n",
    "b3 = np.random.rand(49,6)      \n",
    "conv3 = Convolution(w3,b3,pad=3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 6, 7, 7)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3.forward(x3).shape # 7x7 output!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52173187-65688c80-27c2-11e9-8af0-511ca22c6628.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "## input volume: 32X32X3 filter 5*5\n",
    "x4 = np.random.rand(1,3,32,32)\n",
    "w4 = np.random.rand(10,3,5,5) # 6filter 5X5 filter\n",
    "b4 = np.random.rand(1024,10) # 32X32 \n",
    "conv4 = Convolution(w4,b4,pad=2, stride=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 32, 32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv4.forward(x4).shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52173225-8f6e7e80-27c3-11e9-878a-f0a7a35c0b23.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride = 1, pad = 0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape # (batchsize, colour, height, weight)\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        #전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        #최댓값(2)\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, x):\n",
    "        dout = dout.transpose(0,2,3,1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.s.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 1.41884088,  1.15983335],\n",
       "         [ 1.99876974, -0.22019957]],\n",
       "\n",
       "        [[ 1.29151128,  1.14541837],\n",
       "         [ 1.35793363,  0.54427693]],\n",
       "\n",
       "        [[ 0.8756976 ,  0.62052864],\n",
       "         [ 2.57428995,  0.7788195 ]]]])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.randn(1,3,4,4)\n",
    "Pool = Pooling(2,2,stride=2) \n",
    "Pool.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[1,1,2,4],\n",
    "                [5,6,7,8],\n",
    "                [3,2,1,0],\n",
    "                [1,2,3,4]]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pool = Pooling(2,2,stride=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[6., 8.],\n",
       "         [3., 4.]]]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pool.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {\n",
    "            'W1': weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size),\n",
    "            'b1': np.zeros(filter_num),\n",
    "            'W2': weight_init_std * np.random.randn(pool_output_size, hidden_size),\n",
    "            'b2': np.zeros(hidden_size),\n",
    "            'W3': weight_init_std * np.random.randn(hidden_size, output_size),\n",
    "            'b3': np.zeros(output_size),\n",
    "        }\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "            \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "            \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "            \n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.layers['Conv1'].dW,\n",
    "            'b1': self.layers['Conv1'].db,\n",
    "            'W2': self.layers['Affine1'].dW,\n",
    "            'b2': self.layers['Affine1'].db,\n",
    "            'W3': self.layers['Affine2'].dW,\n",
    "            'b3': self.layers['Affine2'].db,\n",
    "        }\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name='params.pkl'):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name='params.pkl'):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# if you want to reduce training time, apply following lines.\n",
    "x_train = x_train[:5000]\n",
    "t_train = t_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "t_test = t_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2995731609903176\n",
      "=== epoch:1, train acc:0.223, test acc:0.197 ===\n",
      "train loss:2.2969068238454633\n",
      "train loss:2.292434397173415\n",
      "train loss:2.2854925535874178\n",
      "train loss:2.27490929461273\n",
      "train loss:2.266689276941388\n",
      "train loss:2.2669428702039927\n",
      "train loss:2.248037432034485\n",
      "train loss:2.226887780492325\n",
      "train loss:2.2080510662512736\n",
      "train loss:2.1736515465778075\n",
      "train loss:2.126932162280348\n",
      "train loss:2.103144222055123\n",
      "train loss:2.0587359987048526\n",
      "train loss:2.001569781615461\n",
      "train loss:1.9363298946922223\n",
      "train loss:1.7673470029529674\n",
      "train loss:1.8032646809871629\n",
      "train loss:1.77526206612391\n",
      "train loss:1.6616556572498298\n",
      "train loss:1.6114805424625935\n",
      "train loss:1.4169475618058829\n",
      "train loss:1.4212768711279977\n",
      "train loss:1.2815824564074838\n",
      "train loss:1.2283695657740141\n",
      "train loss:1.1590456353665288\n",
      "train loss:1.029974848252532\n",
      "train loss:1.006837707656893\n",
      "train loss:0.9172189970478394\n",
      "train loss:0.9914255228465175\n",
      "train loss:0.978259520707926\n",
      "train loss:0.7780486914101309\n",
      "train loss:0.7599510196166743\n",
      "train loss:0.6769680140023424\n",
      "train loss:0.7623920382313725\n",
      "train loss:0.68483913951842\n",
      "train loss:0.714081058095956\n",
      "train loss:0.6908937548838371\n",
      "train loss:0.681113864289891\n",
      "train loss:0.5012864419997599\n",
      "train loss:0.6956378875975993\n",
      "train loss:0.6188831524681843\n",
      "train loss:0.5671243770255093\n",
      "train loss:0.5020368644582244\n",
      "train loss:0.4093793029565266\n",
      "train loss:0.6454610791733728\n",
      "train loss:0.6312895499297907\n",
      "train loss:0.3942009775656515\n",
      "train loss:0.4267772917594384\n",
      "train loss:0.633031432088808\n",
      "train loss:0.6331319913211969\n",
      "=== epoch:2, train acc:0.816, test acc:0.801 ===\n",
      "train loss:0.6557823710570614\n",
      "train loss:0.4238513060612138\n",
      "train loss:0.617062510540274\n",
      "train loss:0.40010659239800433\n",
      "train loss:0.4837082512029089\n",
      "train loss:0.47548539923579336\n",
      "train loss:0.643335068598154\n",
      "train loss:0.4592242801522199\n",
      "train loss:0.49534485641306913\n",
      "train loss:0.3181045896228751\n",
      "train loss:0.407335984363483\n",
      "train loss:0.4563892608391126\n",
      "train loss:0.6378111633894478\n",
      "train loss:0.506778302595112\n",
      "train loss:0.5936522673345628\n",
      "train loss:0.5106511739242007\n",
      "train loss:0.4652260311428858\n",
      "train loss:0.6376619806886581\n",
      "train loss:0.4988738801641053\n",
      "train loss:0.5628712597217619\n",
      "train loss:0.4399200018449763\n",
      "train loss:0.44304587303087684\n",
      "train loss:0.4261914414171424\n",
      "train loss:0.3804943531397612\n",
      "train loss:0.4809036405734058\n",
      "train loss:0.39081407469994306\n",
      "train loss:0.4527099010060322\n",
      "train loss:0.5184803100268208\n",
      "train loss:0.3588201063914161\n",
      "train loss:0.6119728873266168\n",
      "train loss:0.3841652473213143\n",
      "train loss:0.398240921817061\n",
      "train loss:0.42913064596688116\n",
      "train loss:0.20502169276706575\n",
      "train loss:0.28677056285042524\n",
      "train loss:0.3683573937144641\n",
      "train loss:0.2960846989296232\n",
      "train loss:0.44526476143144145\n",
      "train loss:0.40826511757950246\n",
      "train loss:0.21056234773588983\n",
      "train loss:0.19432592234076065\n",
      "train loss:0.316291073905111\n",
      "train loss:0.300884247658993\n",
      "train loss:0.37313605677745054\n",
      "train loss:0.40258467648781265\n",
      "train loss:0.25721055960692113\n",
      "train loss:0.21769304192196604\n",
      "train loss:0.5168782432262381\n",
      "train loss:0.3542316350365653\n",
      "train loss:0.4510111723384966\n",
      "=== epoch:3, train acc:0.872, test acc:0.855 ===\n",
      "train loss:0.2272870862770486\n",
      "train loss:0.3074010419723941\n",
      "train loss:0.28839411599089343\n",
      "train loss:0.4855960060307323\n",
      "train loss:0.2245278959164936\n",
      "train loss:0.27439930689072184\n",
      "train loss:0.5643309110852021\n",
      "train loss:0.3162821232209809\n",
      "train loss:0.38752123213555106\n",
      "train loss:0.15827410970044054\n",
      "train loss:0.3353500824224808\n",
      "train loss:0.24061338624858414\n",
      "train loss:0.23926251012022617\n",
      "train loss:0.4236213523592991\n",
      "train loss:0.2210850136395813\n",
      "train loss:0.2582087120238579\n",
      "train loss:0.31047465233105936\n",
      "train loss:0.3850544750418808\n",
      "train loss:0.3600908648579291\n",
      "train loss:0.22936275865580416\n",
      "train loss:0.26688455638567\n",
      "train loss:0.2752526409469897\n",
      "train loss:0.3249466471674792\n",
      "train loss:0.22508180402952413\n",
      "train loss:0.3183477676727313\n",
      "train loss:0.3191544015466844\n",
      "train loss:0.24298066500288645\n",
      "train loss:0.31878515873346097\n",
      "train loss:0.2387006915740338\n",
      "train loss:0.2823923715888215\n",
      "train loss:0.34807686328541204\n",
      "train loss:0.2428791425860507\n",
      "train loss:0.419120955458622\n",
      "train loss:0.21172652743808915\n",
      "train loss:0.14685977434525455\n",
      "train loss:0.380112205481908\n",
      "train loss:0.27606892946884487\n",
      "train loss:0.3009347093162064\n",
      "train loss:0.18807428759799735\n",
      "train loss:0.23202849291667055\n",
      "train loss:0.4402942018085034\n",
      "train loss:0.2891265975578791\n",
      "train loss:0.42836489886730805\n",
      "train loss:0.1625463041203665\n",
      "train loss:0.21563147733150348\n",
      "train loss:0.30781179695885896\n",
      "train loss:0.3106099625059804\n",
      "train loss:0.2460146121753249\n",
      "train loss:0.23426272674368925\n",
      "train loss:0.27223542104161846\n",
      "=== epoch:4, train acc:0.891, test acc:0.89 ===\n",
      "train loss:0.28846269035252914\n",
      "train loss:0.2525374406077294\n",
      "train loss:0.3008181238927135\n",
      "train loss:0.3145370286209016\n",
      "train loss:0.326976359595399\n",
      "train loss:0.2748363151143875\n",
      "train loss:0.20137942408611426\n",
      "train loss:0.3597046489714904\n",
      "train loss:0.23937974561900702\n",
      "train loss:0.3552110105233519\n",
      "train loss:0.3937184085171392\n",
      "train loss:0.2093830687773311\n",
      "train loss:0.1948502135362562\n",
      "train loss:0.31864054820331295\n",
      "train loss:0.2835838158710315\n",
      "train loss:0.25616200954036483\n",
      "train loss:0.18805926670490172\n",
      "train loss:0.34906451995696147\n",
      "train loss:0.23875491029004198\n",
      "train loss:0.3388443141398074\n",
      "train loss:0.4881569305720892\n",
      "train loss:0.29044941221329035\n",
      "train loss:0.23210150561923157\n",
      "train loss:0.16570252072909908\n",
      "train loss:0.3765167794709904\n",
      "train loss:0.1992545312827228\n",
      "train loss:0.21073985841181664\n",
      "train loss:0.20317605433694527\n",
      "train loss:0.25527279606186903\n",
      "train loss:0.1917118581740697\n",
      "train loss:0.5760265370856006\n",
      "train loss:0.17187085116069628\n",
      "train loss:0.36258438129112264\n",
      "train loss:0.2161671388636517\n",
      "train loss:0.34166103976600276\n",
      "train loss:0.16627557320269556\n",
      "train loss:0.18170242546902543\n",
      "train loss:0.21218460506707065\n",
      "train loss:0.1673978282478864\n",
      "train loss:0.2125513906001616\n",
      "train loss:0.2538975381133377\n",
      "train loss:0.20951686350844487\n",
      "train loss:0.21268485382626387\n",
      "train loss:0.40170280986061646\n",
      "train loss:0.2183865967143937\n",
      "train loss:0.1988963158650889\n",
      "train loss:0.19341662779067056\n",
      "train loss:0.2749934748873667\n",
      "train loss:0.18250973549704422\n",
      "train loss:0.26169039036390435\n",
      "=== epoch:5, train acc:0.905, test acc:0.908 ===\n",
      "train loss:0.2072746418981113\n",
      "train loss:0.2593452780658806\n",
      "train loss:0.2452009278149706\n",
      "train loss:0.2214808909638461\n",
      "train loss:0.2153933181424619\n",
      "train loss:0.2593833938114509\n",
      "train loss:0.18024353621801315\n",
      "train loss:0.32002031385766605\n",
      "train loss:0.19176551179140625\n",
      "train loss:0.4661835197691282\n",
      "train loss:0.296932646953585\n",
      "train loss:0.20287681112979927\n",
      "train loss:0.2882017217844369\n",
      "train loss:0.17405976678959156\n",
      "train loss:0.23931021342842662\n",
      "train loss:0.30641904455443897\n",
      "train loss:0.19665685374095498\n",
      "train loss:0.15625181545916836\n",
      "train loss:0.21304658102200846\n",
      "train loss:0.24791773555835278\n",
      "train loss:0.24658473145014984\n",
      "train loss:0.11094592078462295\n",
      "train loss:0.3046006509656502\n",
      "train loss:0.13642827969061977\n",
      "train loss:0.31792175994177746\n",
      "train loss:0.16223817203164548\n",
      "train loss:0.14900800449535181\n",
      "train loss:0.17913686462874803\n",
      "train loss:0.30365524297761426\n",
      "train loss:0.22436468923285116\n",
      "train loss:0.11532030678267699\n",
      "train loss:0.17675106926529605\n",
      "train loss:0.09803948285075395\n",
      "train loss:0.1951316047823159\n",
      "train loss:0.1377277919169308\n",
      "train loss:0.120444222934017\n",
      "train loss:0.1851943142587207\n",
      "train loss:0.1909056614733452\n",
      "train loss:0.09008765095818379\n",
      "train loss:0.12419826219306537\n",
      "train loss:0.16674896720271257\n",
      "train loss:0.2960775448555932\n",
      "train loss:0.2990815881784096\n",
      "train loss:0.2520270381560931\n",
      "train loss:0.22455093510859933\n",
      "train loss:0.3684056692705351\n",
      "train loss:0.2873962616630676\n",
      "train loss:0.14918404871221125\n",
      "train loss:0.32785327004014\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.906\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 5\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),\n",
    "                        conv_param={'filter_num': 30,\n",
    "                                    'filter_size': 5,\n",
    "                                    'pad': 0,\n",
    "                                    'stride': 1,\n",
    "                                   }\n",
    "                       )\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000,\n",
    "                 )\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
