{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loss_Function\n",
    "---\n",
    "랜덤으로 정해지는 W. 따라서 Loss를 최소화하는 W를 찾아야 한다.\n",
    "\n",
    "Loss는 모델에 의한 예측 이미지 라벨과 실제이미지의 라벨이 일치하는 정도를 의미한다. \n",
    "\n",
    "전체 데이터셋의 Loss는 다음과 같이 표현할 수 있다. \n",
    "\n",
    "## $L = 1/N\\sum_{i}^{N}L_{i}(f(x_{i},W),y_{i})$\n",
    "\n",
    "$f(x_{i},W)$ 는 모델에 의해 예측된 이미지 라벨, $y_{i}$는 실제 라벨을 말한다. Loss를 모두 합쳐 나누면, 전체 데이터셋의 Loss를 구할 수 있다. \n",
    "\n",
    "Loss를 측정하는 방법으로 두가지를 소개합니다\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) SVM_LOSS\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "- SVM Loss란?\n",
    "\n",
    "$L_{i} = \\sum_{j \\not\\equiv y_{i}}max(0,s_{j}-s_{y_{i}}+1)$\n",
    "\n",
    "true score 즉 $s_{y_{i}}$ 가 다른 $s_{j}$와 비교했을때 safety margin보다 높으면\n",
    "Loss가 떨어지고 낮으면 Loss가 생김. \n",
    "\n",
    "예를 들어 첫번째 고양이 사진을 봤을 때, car가 5.2로 score가 가장 높다. 따라서 car와 cat사이에 1.9 + 1 = 2.9 의 loss가 생긴다.\n",
    "\n",
    "근사한 차이의 경우 잘못 분류할 수 있으므로 safety margin을 설정하여 더 큰 차이로 분류를 해냈을때 성공(Loss = 0)으로 정의한다. safety margin은 반드시 1일 필요는 없다\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52391717-12ece000-2ae2-11e9-8789-057790ae606b.PNG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "- __Q&A 를 통한 SVM Loss이해하기__\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Q1.  __Loss의 최솟값, 최댓값은 무엇인가?__\n",
    "\n",
    "A1. 최솟값은 0이며, 최댓값은 Infinity\n",
    "\n",
    "\n",
    "\n",
    "Q2. __만약 상단 Car점수가 조금 올라간다면 Loss에는 차이가 있는가?__\n",
    "\n",
    "A2. Car점수가 가장 높기 때문에 Loss에는 차이가 없다.\n",
    "\n",
    "\n",
    "\n",
    "Q3. __각 라벨별 score가 0에 가까운 수이면 loss의 값은(safety margin이 1)?__\n",
    "\n",
    "A3. Number of classes -1 ($s_{j} - s_{y_{i}}$가 0이므로)\n",
    "\n",
    "\n",
    "Q4. __loss값을 구하는데 모든 class를 다 더하면 (Including j = y_i)?__\n",
    "\n",
    "\n",
    "A4. safety margin 만큼 loss가 커진다\n",
    "\n",
    "\n",
    "Q5. __Loss 값을 구하는 식에서  mean 대신에 sum을 쓴다면??__\n",
    "\n",
    "\n",
    "A5. rescaling이라서 둘다 써도 상관없음\n",
    "\n",
    "\n",
    "Q6. __Loss값을 구하는 식에서 제곱이 들어간다면?__\n",
    "$L_{i} = \\sum_{j \\not\\equiv y_{i}}max(0,s_{j}-s_{y_{i}}+1)^2$\n",
    "\n",
    "A6. 제곱을 한다는 것은 Loss의 크기에 따라 가중치가 들어감. 조금 나쁜 것과 나쁜 것의 차이를 크게 하여 Loss의 중요도를 조절.\n",
    "\n",
    "Q7. __L=0인 Weight가 유일한가?__\n",
    "\n",
    "A7. 아니다. 2\\*W도 L=0이다. \n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8999999999999995\n",
      "0.0\n",
      "12.9\n"
     ]
    }
   ],
   "source": [
    "def SVM_Loss(scores, y, safety_margin=1): \n",
    "    \"\"\"\n",
    "    scores : w.dot(x)\n",
    "    y : 라벨\n",
    "    safety_margin : safety_margin\n",
    "    \"\"\"\n",
    "    margins = np.maximum(0,scores - scores[y] + safety_margin)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "\n",
    "\n",
    "scores1 = np.array([3.2, 5.1, -1.7])\n",
    "scores2 =  np.array([1.3, 4.9, 2.0])\n",
    "scores3 = np.array([2.2, 2.5, -3.1])\n",
    "\n",
    "y0 = 0\n",
    "y1 = 1\n",
    "y2 = 2\n",
    "\n",
    "\n",
    "print(SVM_Loss(scores1,y0)) # 2.89\n",
    "print(SVM_Loss(scores2,y1)) # 0\n",
    "print(SVM_Loss(scores3,y2)) # 12.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52476823-3ba8ce80-2be3-11e9-9bce-a8e1aa5654e9.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "- Regularization : 모델 복잡도에 대한 페널티\n",
    "\n",
    "$\\lambda$ = regularization strength\n",
    "\n",
    "training_data에 overfitting되는 것을 방지하기 위해서 Regularization을 둔다.\n",
    "\n",
    "람다가 클수록 모델 복잡도에 대한 중요도를 크게 본다.(__Overfitting 방지__)\n",
    "\n",
    "람다가 작다면 고차원의 모델을 필요로 하는것 (__Underfitting 방지__)\n",
    "\n",
    "L1 regression model을 사용하는 regression model을 __Lasso Regression__,\n",
    "\n",
    "L2 regression model을 사용하는 regression model을 __Ridge Regression__ 이라고 부른다\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52479637-a3174c00-2bec-11e9-9c7e-789cf7115746.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "R(W) 가 L2 Regularization이고, x값(이미지픽셀)이 [1,1,1,1]이면 w1[1,0,0,0] 과 w2[0.25,0.25,0.25,0.25] 가 각각 가중치라고 했을때 dot product를 하면 1이라는 똑같은 값이 나온다. \n",
    "\n",
    "L2 Regularization의 입장에서 봤을때 w1과 w2중 어떤 w값을 더 간결하다고 생각한다면 w2가 더 좋을 것이다. w1의 모든 원소의 제곱보다는 w1의 모든 원소의 곱이 더 작다. \n",
    "\n",
    "즉 L2는 w값이 최대한 평평하게 너무 큰 값없이 최대한 비슷한 값으로 이루어지게 하고 싶은 것이다. \n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Softmax\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "Softmax Classifier의 Loss는 다음과 같이 정의된다.\n",
    "\n",
    "$L_{i} = -log(\\frac{e^{s_{y_{i}}}}{\\sum_{j}e^{s_{j}}})$\n",
    "\n",
    "\n",
    "log안에 있는 부분은 제대로 이미지를 분류할 확률이다.\n",
    "\n",
    "이미지의 라벨을 올바르게 분류했을 경우 log안에 들어있는 부분(확률)은 1에 가까워 \n",
    "loss는 0이 나온다. \n",
    "\n",
    "하지만 log값이 0에 가까워지면 loss는 infinity로 향한다 \n",
    "\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52392080-b12d7580-2ae3-11e9-88fc-d2cf4ef9f5ff.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12998254 0.86904954 0.00096793]\n"
     ]
    }
   ],
   "source": [
    "def softmax(a): # 지수함수를 계산할 때 값이 커지면 nan이 계산됨\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c) # 소프트 맥스의 지수함수를 계산할 떄 어떤 정수를 더해도 결과는 바뀌지 않는다는 점을 이용\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "\n",
    "    return y\n",
    "\n",
    "a = [3.2, 5.1, -1.7]\n",
    "\n",
    "print(softmax(a)) # 0.12998, 0.869049 0.000967"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "- __Q&A 를 통한 Softmax Loss이해하기__\n",
    "\n",
    "Q1. __Softmax loss의 min값과 max값은?__\n",
    "\n",
    "A1. SVM과 똑같다. min값은 0이고 max값은 infinity이다.(unbounded)\n",
    "\n",
    "Q2. __W가 너무 작아서 score값이 0에 가깝다면 loss값은?__\n",
    "\n",
    "A2. -log(1/c) = log(c)       # c: class 개수\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52392458-3b2a0e00-2ae5-11e9-924a-9d7b243407ba.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58\n"
     ]
    }
   ],
   "source": [
    "scores4 = np.array([-2.85, 0.86, 0.28])\n",
    "y4 = 2\n",
    "print(SVM_Loss(scores4,y4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0401902864545256"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy_error(y, t): # t는 라벨\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t* np.log(y + delta)) # np.log는 자연로그\n",
    "\n",
    "t0 = [0,0,1]\n",
    "cross_entropy_error(softmax(scores4),t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52392832-f30beb00-2ae6-11e9-9625-b8bdf1679597.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "- __Q&A 를 통한 Softmax Loss이해하기__\n",
    "\n",
    "Q3. __score값을 미세하게 조정한다면 Softmax와 SVM에서 loss가 어떻게 바뀔까?__\n",
    "\n",
    "A3. SVM은 똑같다. SVM에서는 잘 분류하기만 하면 그 크기나 변화는 반영이 안된다. 반면 Softmax에서는 항상 그 값이 갱신된다. (improve, worsen을 항상 반영)\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0\n",
      "--------------------------------------------------\n",
      "0.0009175049577967678\n",
      "0.0008302195426778567\n"
     ]
    }
   ],
   "source": [
    "## svm\n",
    "t0 = 0\n",
    "\n",
    "print(SVM_Loss(np.array([10,-2,3]),0))\n",
    "print(SVM_Loss(np.array([10.01,-2,3]),0))\n",
    "\n",
    "print('-'*50)\n",
    "\n",
    "## Softmax\n",
    "t0 = [1,0,0]\n",
    "\n",
    "print(cross_entropy_error(softmax([10,-2,3]),t0))\n",
    "print(cross_entropy_error(softmax([10.1,-2,3]),t0)) # 값이 조정된것 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "- Recap\n",
    "여태까지 배운 내용들을 정리한것입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52481566-31420100-2bf2-11e9-8de5-bbeaac93f31b.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "앞에서 배운 loss_function을 최소화하는 과정을 optimization이라고 한다.\n",
    "\n",
    "산에서 내려오는 상황에 비유한다면 산의 높이는 loss이고 나의 위치는 w이다.\n",
    "optimization은 loss가 적은 곳으로 w를 변화시키며 내려오는 과정일 것이다.\n",
    "\n",
    "경사가 높은 쪽으로 이동한다면 빨리 낮은곳에 도착할 수 있습니다. \n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52393533-cdccac00-2ae9-11e9-861f-460c4c8dbf85.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "numerical한 방식으로 계산하는것은 느리고 정확하지 않아서 analytic한 방법을 사용할 것이다.(다음장에서)\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "## Numerical_diff\n",
    "def numerical_diff(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "x = np.arange(0,20,0.1)\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.title(\"f(x) = 0.01x^2 + 0.1x\")\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n",
    "print(numerical_diff(function_1,5)) # 0.199999\n",
    "print(numerical_diff(function_1,10)) # 0.299999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.00005 4.00005]\n",
      "[4.99999997e-05 2.00005000e+00]\n"
     ]
    }
   ],
   "source": [
    "## Partial_dff 이런식으로 구하면 속도가 굉장히 느리다. \n",
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val # f(x-h) 계산\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "\n",
    "    return grad\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 또는 return np.sum(x**2)\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0,4.0]))) # 3.00005, 4.00005\n",
    "print(numerical_gradient(function_2, np.array([0.0,2.0]))) # 4.99999-e5 2.000005"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
