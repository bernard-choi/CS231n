{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Convnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},                # Conv layer는 30X5X5의 filter, pad는 0, stride=1의 과정을 거친다.\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']                                                           # 앞에서 설정한 parameter를 받는다. \n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        self.params = {\n",
    "            'W1': weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size), # (1,28,28) 의 필터가 30개 \n",
    "            'b1': np.zeros(filter_num),\n",
    "            'W2': weight_init_std * np.random.randn(pool_output_size, hidden_size),                      # FC Layer\n",
    "            'b2': np.zeros(hidden_size),\n",
    "            'W3': weight_init_std * np.random.randn(hidden_size, output_size),                           # 최종 FC Layer\n",
    "            'b3': np.zeros(output_size),\n",
    "        }\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],                        # Conv - Pool - FC - FC의 단계를 거침\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)                                    # Pooling의 filter_size는 2X2 , stride는 2\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "            \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "            \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "            \n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {\n",
    "            'W1': self.layers['Conv1'].dW,\n",
    "            'b1': self.layers['Conv1'].db,\n",
    "            'W2': self.layers['Affine1'].dW,\n",
    "            'b2': self.layers['Affine1'].db,\n",
    "            'W3': self.layers['Affine2'].dW,\n",
    "            'b3': self.layers['Affine2'].db,\n",
    "        }\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2990969968010684\n",
      "=== epoch:1, train acc:0.198, test acc:0.144 ===\n",
      "train loss:2.297184019797477\n",
      "train loss:2.292912544438652\n",
      "train loss:2.2848313386574755\n",
      "train loss:2.275473874606533\n",
      "train loss:2.2661193208885724\n",
      "train loss:2.255243052341911\n",
      "train loss:2.2384072685013816\n",
      "train loss:2.214533271934115\n",
      "train loss:2.1724806381585786\n",
      "train loss:2.1568223395102213\n",
      "train loss:2.120980418277485\n",
      "train loss:2.071596608100877\n",
      "train loss:2.0192508992737705\n",
      "train loss:1.9423291688173856\n",
      "train loss:1.902918858098832\n",
      "train loss:1.865158325082916\n",
      "train loss:1.6930826367291079\n",
      "train loss:1.5983910900084424\n",
      "train loss:1.6686209485074182\n",
      "train loss:1.4945287207873519\n",
      "train loss:1.485546704151108\n",
      "train loss:1.4226618448982964\n",
      "train loss:1.3454169933982256\n",
      "train loss:1.2041835086066015\n",
      "train loss:1.1092696006784823\n",
      "train loss:1.0524520448267016\n",
      "train loss:1.029744004826984\n",
      "train loss:0.951195784217169\n",
      "train loss:0.9777316571881048\n",
      "train loss:0.8754994652519631\n",
      "train loss:0.8110061805339832\n",
      "train loss:0.7658473998291625\n",
      "train loss:0.6738771718252871\n",
      "train loss:0.7717674302302782\n",
      "train loss:0.8268950073327356\n",
      "train loss:0.7087583337123924\n",
      "train loss:0.6145820768418429\n",
      "train loss:0.6775796346416564\n",
      "train loss:0.4333036391332361\n",
      "train loss:0.5149348986285376\n",
      "train loss:0.6689811960249727\n",
      "train loss:0.6356558955439133\n",
      "train loss:0.46186810561405656\n",
      "train loss:0.6694369657942157\n",
      "train loss:0.7260565304449135\n",
      "train loss:0.5662820420649892\n",
      "train loss:0.6897846162011256\n",
      "train loss:0.5563149503387451\n",
      "train loss:0.5353129033216514\n",
      "train loss:0.44990789173486545\n",
      "=== epoch:2, train acc:0.793, test acc:0.793 ===\n",
      "train loss:0.6559124363501563\n",
      "train loss:0.6728866065829427\n",
      "train loss:0.45534830962574807\n",
      "train loss:0.504755195454005\n",
      "train loss:0.3907663540468087\n",
      "train loss:0.4142361990147848\n",
      "train loss:0.6516277752121864\n",
      "train loss:0.26703557083890495\n",
      "train loss:0.447628323742681\n",
      "train loss:0.3885839617566378\n",
      "train loss:0.4932620291416676\n",
      "train loss:0.42948770240096334\n",
      "train loss:0.6937697060482602\n",
      "train loss:0.40722834294743054\n",
      "train loss:0.4413088745738246\n",
      "train loss:0.4652094898356044\n",
      "train loss:0.44804558857987636\n",
      "train loss:0.6563914605205902\n",
      "train loss:0.5635355720835553\n",
      "train loss:0.412419121181448\n",
      "train loss:0.5505190747724471\n",
      "train loss:0.5943221211052353\n",
      "train loss:0.30468478112539094\n",
      "train loss:0.3075545672627302\n",
      "train loss:0.5652225399077516\n",
      "train loss:0.6727021724446719\n",
      "train loss:0.577202224363951\n",
      "train loss:0.47676050574582624\n",
      "train loss:0.36310878336414765\n",
      "train loss:0.577025052629159\n",
      "train loss:0.46209132724477664\n",
      "train loss:0.45554275758461465\n",
      "train loss:0.31776577951204776\n",
      "train loss:0.2866406979509181\n",
      "train loss:0.4289760344713692\n",
      "train loss:0.35419236766453144\n",
      "train loss:0.29635308812577105\n",
      "train loss:0.3138917106078999\n",
      "train loss:0.2915235342973215\n",
      "train loss:0.3609799204210023\n",
      "train loss:0.35696935476399666\n",
      "train loss:0.39209652291260144\n",
      "train loss:0.445656718693399\n",
      "train loss:0.4273772291870992\n",
      "train loss:0.43395449766587246\n",
      "train loss:0.2308526198200888\n",
      "train loss:0.412148477800583\n",
      "train loss:0.4298730475005195\n",
      "train loss:0.39928866831326937\n",
      "train loss:0.2145883890321292\n",
      "=== epoch:3, train acc:0.873, test acc:0.861 ===\n",
      "train loss:0.3113899488431363\n",
      "train loss:0.40212780192818853\n",
      "train loss:0.274135540897616\n",
      "train loss:0.3153535856861733\n",
      "train loss:0.2902342965513248\n",
      "train loss:0.3511626457549245\n",
      "train loss:0.45393015569305517\n",
      "train loss:0.2692913702123805\n",
      "train loss:0.22147293072568014\n",
      "train loss:0.3300631719438393\n",
      "train loss:0.18998572045366743\n",
      "train loss:0.3509385357235529\n",
      "train loss:0.37889830700751853\n",
      "train loss:0.4867384749663565\n",
      "train loss:0.4273395211280166\n",
      "train loss:0.25109172255169177\n",
      "train loss:0.38702394946363533\n",
      "train loss:0.4096330886376958\n",
      "train loss:0.3261981534422807\n",
      "train loss:0.37542933743720036\n",
      "train loss:0.24678801972751946\n",
      "train loss:0.2850052192628777\n",
      "train loss:0.2926885263649558\n",
      "train loss:0.3171809802837044\n",
      "train loss:0.31488817594930096\n",
      "train loss:0.28994739878808906\n",
      "train loss:0.2563286490592702\n",
      "train loss:0.18149073651205064\n",
      "train loss:0.3180601332040718\n",
      "train loss:0.2693469493261053\n",
      "train loss:0.2350278696072864\n",
      "train loss:0.3510515485079962\n",
      "train loss:0.27949690032845986\n",
      "train loss:0.2501794782490352\n",
      "train loss:0.21561633794314908\n",
      "train loss:0.35065862882358767\n",
      "train loss:0.2036891397099153\n",
      "train loss:0.31071073105833386\n",
      "train loss:0.15037318048930595\n",
      "train loss:0.3372428514763939\n",
      "train loss:0.20221262489234146\n",
      "train loss:0.36070240637806544\n",
      "train loss:0.21296324775624265\n",
      "train loss:0.3320162881858835\n",
      "train loss:0.2750298786419091\n",
      "train loss:0.19641144538375202\n",
      "train loss:0.22958164654213215\n",
      "train loss:0.23117343823481648\n",
      "train loss:0.3185314148271181\n",
      "train loss:0.45424008493805174\n",
      "=== epoch:4, train acc:0.89, test acc:0.879 ===\n",
      "train loss:0.29277440726827675\n",
      "train loss:0.4403462669558182\n",
      "train loss:0.34988020356426586\n",
      "train loss:0.19886798732716218\n",
      "train loss:0.2695793444628806\n",
      "train loss:0.4972592368271221\n",
      "train loss:0.2529150630290841\n",
      "train loss:0.3784974216656019\n",
      "train loss:0.2677588767001099\n",
      "train loss:0.22426006636166235\n",
      "train loss:0.15205411094307922\n",
      "train loss:0.26553098392505076\n",
      "train loss:0.44971009203013634\n",
      "train loss:0.21597110876381273\n",
      "train loss:0.5098334366617796\n",
      "train loss:0.21510186182917773\n",
      "train loss:0.2783547835552395\n",
      "train loss:0.28202939562219437\n",
      "train loss:0.2707883636472871\n",
      "train loss:0.300066836975189\n",
      "train loss:0.3872452070655369\n",
      "train loss:0.29579062540314\n",
      "train loss:0.19566522506769832\n",
      "train loss:0.25287742000911895\n",
      "train loss:0.17819260514368587\n",
      "train loss:0.23250714140566703\n",
      "train loss:0.24694053687044298\n",
      "train loss:0.26149165686715653\n",
      "train loss:0.2199755948857066\n",
      "train loss:0.3814440450973582\n",
      "train loss:0.20708516711454977\n",
      "train loss:0.39270211014444867\n",
      "train loss:0.21977543269712163\n",
      "train loss:0.3524483869611677\n",
      "train loss:0.45920266054909026\n",
      "train loss:0.18551771109974127\n",
      "train loss:0.24041426390223106\n",
      "train loss:0.23961580413432304\n",
      "train loss:0.2894411399321726\n",
      "train loss:0.18132756336195333\n",
      "train loss:0.36926359004478215\n",
      "train loss:0.22051470562763434\n",
      "train loss:0.2519954110849094\n",
      "train loss:0.2385438576394994\n",
      "train loss:0.20937680405276818\n",
      "train loss:0.2787603044268759\n",
      "train loss:0.23066371964858118\n",
      "train loss:0.353731906475449\n",
      "train loss:0.2502456255213393\n",
      "train loss:0.1443853825399958\n",
      "=== epoch:5, train acc:0.914, test acc:0.901 ===\n",
      "train loss:0.3098199424911695\n",
      "train loss:0.2797558091839136\n",
      "train loss:0.26215831555715313\n",
      "train loss:0.27111450428654016\n",
      "train loss:0.19795374923557352\n",
      "train loss:0.22251413748475596\n",
      "train loss:0.11940938095848458\n",
      "train loss:0.4333502021713831\n",
      "train loss:0.26278551211245144\n",
      "train loss:0.2047064952901521\n",
      "train loss:0.1492037829109554\n",
      "train loss:0.3636435980225163\n",
      "train loss:0.14293903480200576\n",
      "train loss:0.18769871150476866\n",
      "train loss:0.2208720668263246\n",
      "train loss:0.16626561570251994\n",
      "train loss:0.18935309487494384\n",
      "train loss:0.24575461567309262\n",
      "train loss:0.3088675778337973\n",
      "train loss:0.18791081929474132\n",
      "train loss:0.20588134818573675\n",
      "train loss:0.35291091560946614\n",
      "train loss:0.10627604325959408\n",
      "train loss:0.34602054440365754\n",
      "train loss:0.17060385063836572\n",
      "train loss:0.2762642686037846\n",
      "train loss:0.15954140998604008\n",
      "train loss:0.2146126108243594\n",
      "train loss:0.2196507998493053\n",
      "train loss:0.26204965295288163\n",
      "train loss:0.14182430093458603\n",
      "train loss:0.22872375772210815\n",
      "train loss:0.20910891751861388\n",
      "train loss:0.24230505221193394\n",
      "train loss:0.2695117830718683\n",
      "train loss:0.13622084366752352\n",
      "train loss:0.4017795595293172\n",
      "train loss:0.20941118338787795\n",
      "train loss:0.3469808833505566\n",
      "train loss:0.2060843299255163\n",
      "train loss:0.22734530175852627\n",
      "train loss:0.27911235536485374\n",
      "train loss:0.4336065570814238\n",
      "train loss:0.18255951890136468\n",
      "train loss:0.2555978163600495\n",
      "train loss:0.23509651886869098\n",
      "train loss:0.20529118698041648\n",
      "train loss:0.14043554027757368\n",
      "train loss:0.15882140792014524\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# if you want to reduce training time, apply following lines.\n",
    "x_train = x_train[:5000]                                                      # 5000개만 가져와서 일부분만 학습 진행\n",
    "t_train = t_train[:5000]\n",
    "x_test = x_test[:1000]\n",
    "t_test = t_test[:1000]\n",
    "\n",
    "\n",
    "\n",
    "max_epochs = 5                                                                # Epoch은 5로 설정\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1, 28, 28),                                # Parameter설정 (28,28,1)의 데이터를 받고 filter_num:30, filter_size: 5\n",
    "                        conv_param={'filter_num': 30,\n",
    "                                    'filter_size': 5,                         # pad:0, stride:1\n",
    "                                    'pad': 0,\n",
    "                                    'stride': 1,\n",
    "                                   }\n",
    "                       )\n",
    " \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,                  # learning rate : 0.001, optimizer= 'Adam'\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000,\n",
    "                 )\n",
    "\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
