{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SGD의 문제점 1 - 학습속도가 느리다__\n",
    "\n",
    "- 최저점을 향하는 과정에서 경사가 가파른 영역으로 방향을 정하기 때문에 오른쪽으로 바로 달려가는 것이 아니라 위아래로 지그재그 움직인다.\n",
    "\n",
    "\n",
    "- step 마다 지그재그로 왔다갔다 해서 매우 느린 속도로 학습이 진행됨.\n",
    "\n",
    "\n",
    "- 고차원에선 이런 문제가 많이 발생한다. 이해가 잘 안되므로 그림을 통해 확인해보자\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53087883-8fa0a500-354b-11e9-9b60-a586952c33c8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53087921-a5ae6580-354b-11e9-8461-040333828f34.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "__SGD의 문제점2 - local minima, saddle point__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53066154-e7221f00-3511-11e9-8d38-0bae89f7c48d.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  local minima\n",
    "\n",
    "local minima가 있으면 gradient가 0이 되어서 빠져나오지 못한다. dimension이 많은 경우 모든 방향에서 loss가 증가해야함. 따라서 local minima는 매우 드물게 나타납니다.\n",
    "\n",
    "### Saddle Point\n",
    "\n",
    "더 내려가야 하지만 gradient가 엄청 작은 값이라 느리게 진행됨. High dimension에서 더 흔히 나타나는데 한방향으로는 loss가 증가하고 한 방향으로는 loss가 내려가기 때문. saddle point 근처도 slope가 평평하여 loss가 0에 가까워 집니다. Local minima보다 자주 나타나는 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "__SGD의 문제점 3 - noisy estimate__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53066536-aaefbe00-3513-11e9-83ec-b7dfd1546257.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini batch를 이용해 gradient에 대한 true information이 아닌 noisy estimate를 얻는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## SGD를 보완하는 여러 Optimization\n",
    "\n",
    "## 1. SGD+Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53066847-fd7daa00-3514-11e9-8bf6-93abefe3a2b4.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD with Momentum은 이전의 속도와 방향성을 유지하여 다시 사용하는 것. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, params, grads):\n",
    "        for key in params.key():\n",
    "            params[key] -= self.lr * grads[key] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    def __init__(self,lr=0.01,momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.item():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        # 초기값 0 \n",
    "                \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]      \n",
    "            params[key] += self.v[key]  \n",
    "            \n",
    "        # self.v는 이전의 움직임을 의미."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53086432-721e0c00-3548-11e9-8f13-c2b6bdf6b5b8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53081103-7e509c00-353d-11e9-9edf-6c44ae79433b.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지금 위치(붉은색 점)에서 gradiet를 계산하는 것이 아니라 Velocity로 옮긴 후 예견된 위치에서 gradient를 구함\n",
    "\n",
    "\n",
    "\n",
    "- Convex optimization 관점에서 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## 3. AdaGrad\n",
    "\n",
    "- velocity 대신 gradient의 제곱(squared_term)을 사용\n",
    "\n",
    "\n",
    "- 많이 바뀌는 gradietn는 큰 수로 나누고  변하지 않는 gradient는 작은 수로 나누어 속도를 빠르게 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53082170-a93bef80-353f-11e9-8801-44de2822b565.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53084139-93302e00-3543-11e9-8e29-38efcd1e0ff8.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr = 0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = {}\n",
    "            for key, val in params.item():\n",
    "                self.h[key] = np.zeros_like(val)\n",
    "                \n",
    "            # parameter개수만큼 0 벡터를 만들어줍니다. 이는 초기값에 활용됩니다. \n",
    "            \n",
    "        for key in params.keys():\n",
    "            self.h[key] += grads[key] * grads[key] \n",
    "            # 초기값은 0이지만 upfate됩니다. \n",
    "            \n",
    "            params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key] + 1e-7)\n",
    "                                                   \n",
    "            # 1e -7이 위 식의 epsilon에 해당됩니다. 분모에 0에 가까운 값이 들어갈때 오류가 날 수 있으므로 이 값을 넣어줍니다.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53089664-88c86100-3550-11e9-8b5b-d1bb2822a394.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- squared_gradient를 축적만 하지 않고 decay rate를 도입\n",
    "\n",
    "\n",
    "- decay rate = 0.9 or 0.09\n",
    "\n",
    "\n",
    "- 강화학습에선 RMSProp을 많이 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53088309-b14e5c00-354c-11e9-897f-657f0c2601cf.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53089141-11de9880-354f-11e9-8cc4-55d164d76d84.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- almost use\n",
    "\n",
    "\n",
    "- velocity와 squared를 모두 사용\n",
    "\n",
    "\n",
    "- first moment: momentum\n",
    "\n",
    "\n",
    "- second moment : AdaGrad / RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53090950-207b7e80-3554-11e9-9090-a68cf1533d12.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53090837-de523d00-3553-11e9-8dff-2626e0a056da.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53091686-f0cd7600-3555-11e9-99f3-839a711aab97.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lr 을 decay시켜 minima에 도달할 수 있도록 설정\n",
    "\n",
    "\n",
    "- 처음에 no decay로 시도해보고 직접 눈으로 확인\n",
    "\n",
    "\n",
    "- decay 방법은 exponential decay, 1/t decay등이 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53091781-3c801f80-3556-11e9-8ddd-7931be7aad6b.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실함수에 가중치의 L2 노름을 더한 가중치 감소는 간단하게 구현할 수 있고 어느정도 지나친 학습을 억제 할 수 있습니다. \n",
    "\n",
    "\n",
    "- 신경망 모델이 복잡해지면 가중치 감소만으로 대응하기 어려워지는데 이럴 때  __드롭아웃__ 이라는 기법을 씁니다.\n",
    "\n",
    "\n",
    "- Prevent co-adaptation(상호작용) of features\n",
    "\n",
    "\n",
    "- Kind of like large ensemble models(that share parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/53114646-a1a03900-3587-11e9-81f3-6080d3863fe6.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train과 Test에서 순전파 과정이 다릅니다. Train에서는 노드의 부분들을 남겨놓았지만 Test에서는 모든 노드를 써야합니다. \n",
    "\n",
    "Train: 만약 1024(n)개의 뉴런을 사용하고 0.5(p)의 dropout비율을 사용한다면 512(n\\*p)개의 뉴런만을 남기게 됩니다.\n",
    "\n",
    "Test: 각 뉴련의 출력에 훈련때 삭제한 비율을 곱하여 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_ratio = 0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        # x와 같은 크기의 mask를 만들고 ratio보다 작은 값들은 0으로 만든다. \n",
    "        # 삭제할 뉴런은 False, 유지되는 뉴런은 True\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        # train과 test의 각각 forward의 과정이 다르다. train에서는 mask를 통해 노드를 죽이는 과정이 있지만\n",
    "        # test에서는 모든 노드를 사용하지만 그 값을 평균낸다. 0,5이면 각 노드의 값을 반으로 나눠준다. \n",
    "        # 1 - self.dropout_ratio : 삭제한 비율\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "        \n",
    "        # 선택되지 않은 노드는 gradient가 0이 전달된다. \n",
    "        # 순전파 때 신호를 통과시키는 뉴런은 역전파때도 신호를 그대로 통과시키고, 순전파때 통과시키지 않는 뉴런은 역전파때도 신호를 차단."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout 과 Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습에서는 앙상블 학습을 애용합니다. 앙상블 학습은 개별적으로 학습시킨 여러 모델의 출력을 평균내어 추론하는 방식입니다.\n",
    "\n",
    "신경망의 맥락에서 얘기하면, 가령 같은 (혹은 비슷한) 구조의 네트워크를 5개 준비하여 따로따로 학습시키고 시험때는 그 5개의 출력을 평균내어 답하는 것이죠. 앙상블 학습을 수행하면 신경망의 정확도가 몇 프로 정도 개선된다는 것이 실험적으로 알려져 있습니다.\n",
    "\n",
    "앙상블 학습은 드롭아웃과 밀접합니다. 드롭아웃이 학습때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델을 학습시키는 것으로 해석할 수 있기 때문이죠. 그리고 츄론 때는 뉴런의 출력에 삭제한 비율을 곱함으로써 앙상블 학습에서 여러 모델의 평균을 내는것과 같은 효과를 얻을 수 있습니다.\n",
    "\n",
    "즉 드롭아웃은 앙상블 학습과 같은 효과를 하나의 네트워크로 구현했다고 생각할 수 있습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
