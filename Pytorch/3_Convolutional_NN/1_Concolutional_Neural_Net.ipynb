{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=84, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size(filter), stride, padding=0, dilation=1, groups=1, bias = True)\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(1,6,5),\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "        \n",
    "        # out_channels = 6이므로 in_channels = 6\n",
    "        # out_channels = 16이고 kernal_size = 5로 해보겠습니다.\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(6,16,5),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "        # torch.nn.MaxPool2d(kernel_size, stride = None,padding = 0, dilation = 1, return_indices=False, ceil_mode = False)\n",
    "        self.maxpool= nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "        \n",
    "        # an affine operation: y = Wx+b (어파인 연산)\n",
    "        # typical cnn에서는 conv layer뒤에 fully connected layer를 붙입니다.\n",
    "        self.fc1 = nn.Sequential(\n",
    "                        nn.Linear(16*5*5,120),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "                        nn.Linear(120,84),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "                        nn.Linear(84,10),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "          \n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "            \n",
    "        # -1은 몇개가 들어오는지 모른다는 뜩\n",
    "        #  num_flat_features 는 1X? 형태로 flat하게 reshape합니다.\n",
    "            \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "            \n",
    "        return x\n",
    "        \n",
    "    # x.size()[0]은 input 갯수이기 때문에 제외합니다. 각각의 input값들을 1X>형태로 reshape합니다.\n",
    "    # torch.randn(5,3,3)을 생각해보면 개수를 의미하는 5를 제외하고 3X3=9의 값을 return합니다.\n",
    "        \n",
    "    def num_flat_features(self,x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "net = Net()\n",
    "print(net)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward 이후에 backward는 autograd를 이용해 자동으로 정의할 수 있습니다.\n",
    "- net의 weight들은 net.parameters()에 의해 return됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # 5X5 행렬 6개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_input = torch.randn(1,1,32,32) # 32X32 하나를 input으로 넣어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0017, 0.0000, 0.1069, 0.0000, 0.0000,\n",
       "         0.0000]], grad_fn=<ThresholdBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.forward(temp_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input이 왜 1X1X32X32로 들어가야할까요? torch.nn에서는 mini-batch만 지원합니다. 따라서 nnconv2d는 nsamples X nChannels X Height X Width의 4차원 tensor를 입력으로 합니다. 1개의 값을 input channel(1)에 넣으니 1X1이 됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 연산의 gradient가 저장되어 있는데 그 값을 초기화 해줍니다.\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10)) # loss를 정의하지 않았으므로 임의의 값으로 back prop을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor는 autograd 연산을 지원합니다. 또한 tensor의 gradient를 갖고 있습니다. back prop할때마다 이를 초기화 해야합니다.\n",
    "nn.module은 weight를 캡슐화해서 GPU로 이동, 내보내기 불러오기등의 작업을 도와줍니다.\n",
    "nn.parameter는 tensor의 종류로 module에 할당될 때 자동으로 weight로 등록됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function(손실함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수는 output, target을 한 쌍의 입력으로 받아 output이 target으로부터 얼마나 떨어져 있는지를 추정하는 값을 계산합니다. output은 net이 계산한 추정값(출력), target은 실제 값(정답)입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.2611, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.arange(1,11,dtype = torch.float)\n",
    "# torch.unsqueeze(target, dim=0)과 같습니다. reshape가 목적\n",
    "target = target.view(1,-1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x00000217152DDCF8>\n",
      "<ThresholdBackward0 object at 0x00000217152DD208>\n"
     ]
    }
   ],
   "source": [
    "# grad_fn.next_functions[]을 통해 gradient function을 추적할 수 있습니다.\n",
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0562,  0.0298,  0.0420,  0.0115,  0.0725,  0.0303])\n",
      "tensor([ 0.0000, -0.0685,  0.1242,  0.1937,  0.0541,  0.0095,  0.0256, -0.0544,\n",
      "         0.0030, -0.1352, -0.0774, -0.0222,  0.0410,  0.1506, -0.0082, -0.0541])\n",
      "tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.5945e-02,  9.1963e-02,\n",
      "         0.0000e+00, -4.3009e-02,  0.0000e+00,  0.0000e+00,  2.4676e-02,\n",
      "         0.0000e+00, -4.6041e-02,  7.4650e-06,  2.5243e-02, -4.7554e-02,\n",
      "        -8.7562e-02,  0.0000e+00,  6.9426e-02,  0.0000e+00,  0.0000e+00,\n",
      "        -2.2287e-02, -5.1680e-02, -4.9050e-02, -9.6443e-02,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.1422e-01, -6.0631e-02, -8.9633e-02,\n",
      "         0.0000e+00, -1.1717e-01, -2.8079e-02,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  2.3731e-02,  5.8281e-02,  9.3793e-04,\n",
      "         3.6191e-02, -1.0880e-03, -2.6754e-02,  0.0000e+00,  8.6505e-02,\n",
      "         9.5071e-02,  0.0000e+00, -1.1215e-02,  3.0236e-03,  7.9526e-02,\n",
      "         1.6917e-02,  0.0000e+00,  6.8972e-02,  1.6253e-02,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00, -1.6844e-02,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00, -8.0065e-02,  0.0000e+00,  9.9334e-03,\n",
      "         2.0822e-02,  0.0000e+00, -1.8921e-04, -2.5938e-02,  1.5486e-01,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0730e-01,  0.0000e+00,\n",
      "        -3.1814e-03,  0.0000e+00,  0.0000e+00,  1.0442e-02,  6.6669e-03,\n",
      "        -1.7617e-03,  6.6246e-04,  1.1099e-01,  0.0000e+00,  0.0000e+00,\n",
      "         4.6969e-02,  0.0000e+00,  0.0000e+00, -1.8636e-02,  0.0000e+00,\n",
      "         2.1626e-02, -1.9718e-02,  6.4230e-03, -7.7321e-03,  0.0000e+00,\n",
      "         0.0000e+00, -5.6461e-02,  5.7406e-02,  8.2910e-02,  0.0000e+00,\n",
      "         0.0000e+00, -8.5632e-02,  0.0000e+00,  5.5680e-02, -4.0312e-02,\n",
      "         8.2022e-03,  0.0000e+00,  0.0000e+00,  0.0000e+00, -4.1098e-02,\n",
      "         0.0000e+00, -1.4126e-02,  0.0000e+00,  0.0000e+00,  4.3806e-02,\n",
      "        -1.3454e-01,  2.2058e-02,  0.0000e+00,  4.7700e-02,  0.0000e+00])\n",
      "tensor([ 0.0000,  0.0000, -0.1718,  0.2860, -0.3447,  0.0000,  0.0000, -0.1339,\n",
      "         0.1133, -0.1325, -0.0792,  0.0000,  0.0000,  0.0000,  0.1585,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1991,  0.0103,\n",
      "        -0.2844,  0.0000,  0.1273,  0.0271,  0.3019, -0.0475,  0.0000, -0.1860,\n",
      "         0.0000,  0.0866, -0.1169, -0.0114,  0.3096, -0.1616,  0.0000, -0.2987,\n",
      "        -0.3554,  0.0197,  0.2197, -0.2847,  0.0000,  0.0412,  0.3077, -0.0483,\n",
      "         0.0000,  0.0000, -0.0860,  0.0138,  0.0000, -0.1195,  0.1183, -0.2162,\n",
      "         0.0000,  0.0000, -0.2175,  0.0000,  0.0000, -0.1755,  0.0000,  0.0131,\n",
      "         0.2251,  0.1881,  0.0000,  0.0000,  0.0000, -0.2340,  0.0000,  0.0000,\n",
      "         0.0000, -0.1430,  0.1713,  0.0000, -0.2161,  0.0000, -0.1024,  0.0000,\n",
      "         0.0000, -0.0212,  0.0000,  0.0000])\n",
      "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -1.3782, -1.5901,\n",
      "         0.0000, -1.9992])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "\n",
    "# nn.Sequential로 여러 function을 묶어뒀을 경우 []를 통해 순서대로 출력할 수 있습니다.\n",
    "print(net.conv1[0].bias.grad)\n",
    "# 값이 모두 0으로 초기화되어 있습니다.\n",
    "\n",
    "loss.backward() # gradient를 계산\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1[0].bias.grad)\n",
    "print(net.conv2[0].bias.grad)\n",
    "print(net.fc1[0].bias.grad)\n",
    "print(net.fc2[0].bias.grad)\n",
    "print(net.fc3[0].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update(가중치 갱신)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.01) # Adam optimizer를 사용, learning rate 는 0.01입니다.\n",
    "\n",
    "# 학습과정(가중치 갱신 과정)은 다음과 같다\n",
    "optimizer.zero_grad() # 기존의 변화도에 대해 누적되는 것을 막기 위해 zero_grad()로 초기화\n",
    "output = net(temp_input)\n",
    "loss = criterion(output, target)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
