{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init ## 초기값 위한 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적으로 python의 class를 이용해 모델을 구현합니다.\n",
    "# 일반적으로 __init__, forward, predict 등으로 이루어져 있습니다.\n",
    "\n",
    "class SimpleLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleLinearRegression, self).__init__()\n",
    "        self.Layer = nn.Linear(1,1) # feature가 1이고 output 역시 1개이기 때문에 (1,1)\n",
    "        \n",
    "    # feed forward\n",
    "    def forward(self,inputs):\n",
    "        x = self.Layer(inputs)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, test_input):\n",
    "        x = self.Layer(test_input)\n",
    "        return x\n",
    "    \n",
    "model = SimpleLinearRegression() # 객체 설정\n",
    "criterion = nn.MSELoss() # regression의 loss function은 MSE\n",
    "\n",
    "# gradient descent로 w, b를 찾고 optimizer는 stochastic descent사용\n",
    "# momentum은 관성을 추가하는 내용. 학습속도를 위해 추가\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    \n",
    "    # y = 2x + 0.1\n",
    "    # input 은 일정하게 1.0, 2.0, 3.0이 들어가고 target은 2.1, 4.1, 6.1입니다.\n",
    "    # 우리가 원하는 것은 w,b가 1000번의 학습을 통해 2와 0.1이 되는 것\n",
    "    \n",
    "    inputs = torch.Tensor([1.0, 2.0, 3.0]).unsqueeze(1)\n",
    "    targets = torch.Tensor([2.1, 4.1, 6.1]).unsqueeze(1)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    # step에서 계산된 gradient로 update하고 지워줘야함(초기화)\n",
    "    # 데이터가 계속 들어오는데 저번에 계산된 gradient를 지워준다. \n",
    "    y_pred = model(inputs)\n",
    "    \n",
    "    loss = criterion(y_pred, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 계산된 gradient를 기반으로 optimizer를 이용해 w와 b를 update\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 ==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Trained Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[2.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 1000 번 학스한 결과 w는 2, b는 0.1\n",
    "# model.parameters() 하면 안에 있는 parameter들 다 출력가능\n",
    "w = list(model.parameters())\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.1000], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(torch.Tensor([10.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non lineart Data에 Linear Model을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "num_data = 1000\n",
    "num_epoch = 1000\n",
    "\n",
    "noise = init.normal(torch.FloatTensor(num_data,1), std=1) \n",
    "                                # 노말분포에서 평균은 1 std 1인 노이즈 텐서 생성\n",
    "x = init.uniform(torch.Tensor(num_data,1),-10,10)\n",
    "                                # -10과 10 사이의 균일분포를 따르는 값 \n",
    "y = 2*x +3\n",
    "y_noise = 2**x + 3*x -2 + noise # y = 2x^2 + 3x -2 (Non_linear_data)\n",
    "                                # y_noise는 라벨이 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37731.6055, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    \n",
    "    model.zero_grad()\n",
    "    y_pred = model(Variable(x))\n",
    "    # 모델 통과해서 output나오는 부분\n",
    "    # output과 label모두 Variable로 감싸줘야함\n",
    "    loss = criterion(y_pred, y_noise)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 계산된 gradient를 기반으로 optimizer를 이용해 w와 b를 update\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 ==0:\n",
    "        print(loss) # loss가 줄어들지 않는다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 모델을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fully connected model with 2 hidden layer\n",
    "## nn.Sequential : 여러개의 층을 하나의 모델로 묶을 수 있다. \n",
    "model = nn.Sequential(\n",
    "        nn.Linear(1,6),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(6,10),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(10,1)\n",
    "        )\n",
    "## Layer를 깊게 가질수록 line이 부드러워진다. \n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(91.7335, grad_fn=<L1LossBackward>)\n",
      "tensor(64.7272, grad_fn=<L1LossBackward>)\n",
      "tensor(56.7965, grad_fn=<L1LossBackward>)\n",
      "tensor(48.1641, grad_fn=<L1LossBackward>)\n",
      "tensor(37.4019, grad_fn=<L1LossBackward>)\n",
      "tensor(27.2410, grad_fn=<L1LossBackward>)\n",
      "tensor(24.4260, grad_fn=<L1LossBackward>)\n",
      "tensor(20.5779, grad_fn=<L1LossBackward>)\n",
      "tensor(24.9046, grad_fn=<L1LossBackward>)\n",
      "tensor(23.8067, grad_fn=<L1LossBackward>)\n",
      "tensor(22.9886, grad_fn=<L1LossBackward>)\n",
      "tensor(16.9259, grad_fn=<L1LossBackward>)\n",
      "tensor(19.5400, grad_fn=<L1LossBackward>)\n",
      "tensor(17.4608, grad_fn=<L1LossBackward>)\n",
      "tensor(15.3876, grad_fn=<L1LossBackward>)\n",
      "tensor(14.7671, grad_fn=<L1LossBackward>)\n",
      "tensor(14.0220, grad_fn=<L1LossBackward>)\n",
      "tensor(14.0493, grad_fn=<L1LossBackward>)\n",
      "tensor(13.7295, grad_fn=<L1LossBackward>)\n",
      "tensor(13.4201, grad_fn=<L1LossBackward>)\n",
      "tensor(12.9549, grad_fn=<L1LossBackward>)\n",
      "tensor(13.1432, grad_fn=<L1LossBackward>)\n",
      "tensor(12.3585, grad_fn=<L1LossBackward>)\n",
      "tensor(14.9334, grad_fn=<L1LossBackward>)\n",
      "tensor(11.9055, grad_fn=<L1LossBackward>)\n",
      "tensor(13.1396, grad_fn=<L1LossBackward>)\n",
      "tensor(12.0485, grad_fn=<L1LossBackward>)\n",
      "tensor(12.7306, grad_fn=<L1LossBackward>)\n",
      "tensor(11.1736, grad_fn=<L1LossBackward>)\n",
      "tensor(13.0807, grad_fn=<L1LossBackward>)\n",
      "tensor(13.7628, grad_fn=<L1LossBackward>)\n",
      "tensor(12.3803, grad_fn=<L1LossBackward>)\n",
      "tensor(10.5902, grad_fn=<L1LossBackward>)\n",
      "tensor(10.6304, grad_fn=<L1LossBackward>)\n",
      "tensor(12.6701, grad_fn=<L1LossBackward>)\n",
      "tensor(10.1675, grad_fn=<L1LossBackward>)\n",
      "tensor(10.3268, grad_fn=<L1LossBackward>)\n",
      "tensor(10.3108, grad_fn=<L1LossBackward>)\n",
      "tensor(10.0251, grad_fn=<L1LossBackward>)\n",
      "tensor(10.1053, grad_fn=<L1LossBackward>)\n",
      "tensor(11.2828, grad_fn=<L1LossBackward>)\n",
      "tensor(11.4828, grad_fn=<L1LossBackward>)\n",
      "tensor(9.8595, grad_fn=<L1LossBackward>)\n",
      "tensor(10.9804, grad_fn=<L1LossBackward>)\n",
      "tensor(9.7770, grad_fn=<L1LossBackward>)\n",
      "tensor(9.2461, grad_fn=<L1LossBackward>)\n",
      "tensor(9.2940, grad_fn=<L1LossBackward>)\n",
      "tensor(10.8315, grad_fn=<L1LossBackward>)\n",
      "tensor(9.1701, grad_fn=<L1LossBackward>)\n",
      "tensor(9.2430, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "\n",
    "    \n",
    "    model.zero_grad()\n",
    "    y_pred = model(Variable(x)\n",
    "\n",
    "    loss = criterion(y_pred, y_noise)\n",
    "    loss.backward()\n",
    "    \n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200==0:\n",
    "        print(loss)  # Loss가 떨어진다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
