{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일반적으로 python의 class를 이용해 모델을 구현합니다.\n",
    "# 일반적으로 __init__, forward, predict 등으로 이루어져 있습니다.\n",
    "\n",
    "class SimpleLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleLinearRegression, self).__init__()\n",
    "        self.Layer = nn.Linear(1,1) # feature가 1이고 output 역시 1개이기 때문에 (1,1)\n",
    "        \n",
    "    # feed forward\n",
    "    def forward(self,inputs):\n",
    "        x = self.Layer(inputs)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, test_input):\n",
    "        x = self.Layer(test_input)\n",
    "        return x\n",
    "    \n",
    "model = SimpleLinearRegression() # 객체 설정\n",
    "criterion = nn.MSELoss() # regression의 loss function은 MSE\n",
    "\n",
    "# gradient descent로 w, b를 찾고 optimizer는 stochastic descent사용\n",
    "# momentum은 관성을 추가하는 내용. 학습속도를 위해 추가\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    \n",
    "    # y = 2x + 0.1\n",
    "    # input 은 일정하게 1.0, 2.0, 3.0이 들어가고 target은 2.1, 4.1, 6.1입니다.\n",
    "    # 우리가 원하는 것은 w,b가 1000번의 학습을 통해 2와 0.1이 되는 것\n",
    "    \n",
    "    inputs = torch.Tensor([1.0, 2.0, 3.0]).unsqueeze(1)\n",
    "    targets = torch.Tensor([2.1, 4.1, 6.1]).unsqueeze(1)\n",
    "    \n",
    "    model.zero_grad()\n",
    "    y_pred = model(inputs)\n",
    "    \n",
    "    loss = criterion(y_pred, targets)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 계산된 gradient를 기반으로 optimizer를 이용해 w와 b를 update\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 ==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check Trained Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[2.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([0.1000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "# 1000 번 학스한 결과 w는 2, b는 0.1\n",
    "w = list(model.parameters())\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20.1000], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(torch.Tensor([10.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non lineart Data에 Linear Model을 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "num_data = 1000\n",
    "num_epoch = 1000\n",
    "\n",
    "noise = init.normal(torch.FloatTensor(num_data,1), std=1)\n",
    "x = init.uniform(torch.Tensor(num_data,1),-10,10)\n",
    "y = 2*x +3\n",
    "y_noise = 2**x + 3*x -2 + noise # y = 2x^2 + 3x -2 (Non_linear_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37731.6055, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n",
      "tensor(19849.1641, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    \n",
    "    model.zero_grad()\n",
    "    y_pred = model(Variable(x))\n",
    "    \n",
    "    loss = criterion(y_pred, y_noise)\n",
    "    loss.backward()\n",
    "    \n",
    "    # 계산된 gradient를 기반으로 optimizer를 이용해 w와 b를 update\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 200 ==0:\n",
    "        print(loss) # loss가 줄어들지 않는다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
