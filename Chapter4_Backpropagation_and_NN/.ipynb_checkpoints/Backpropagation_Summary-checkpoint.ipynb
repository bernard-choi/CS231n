{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Back Propagation\n",
    "\n",
    "Loss_function을 최소화하는 weight를 찾기 위해서는 gradient를 계산해야한다.\n",
    "\n",
    "Gradient를 계산하는 방법에는 Numerical gradient(아주 작은 차분으로 미분)와 Analytic gradient(수식을 전개하여 미분)가 있다. \n",
    "\n",
    "오차역전파법을 이용하면 매개변수가 많아도 효율적으로 계산할 수 있어서 앞으로는 Analytic grdient를 사용할 것. \n",
    "\n",
    "그렇다고 수치미분이 필요없는 것이 아니라 오차 역전파법을 정확히 구현했는지 확인하기 위해 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52531322-a2022e00-2d56-11e9-87ce-2f32240e97cd.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:2.446415946928808e-13\n",
      "b1:1.0908033005498086e-12\n",
      "W2:8.834956069281351e-13\n",
      "b2:1.201261340399995e-10\n"
     ]
    }
   ],
   "source": [
    "# TwoLyaernet은 numpy로 구현된 2layer network. 수치미분과 백프로파 비교.\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기 \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 차이의 절댓값을 구한 후, 그 절대값들의 평균을 낸다\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key +':' + str(diff)) # 수치미분과 Anlaytic gradient가 별 차이 없다. 오차역전파법이 실수없이 구현된것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52547028-8b67df80-2e07-11e9-90c9-205cbf31ef15.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "간단한 예제를 통해 Chain Rule이 어떻게 작용되는지 알아보자.\n",
    "\n",
    "우리가 알고 싶은 것은 $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$  \n",
    "\n",
    "즉 x, y, z값이 마지막 output값에 끼치는 영향을 알고 싶다.\n",
    "&nbsp;\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52547036-9753a180-2e07-11e9-8670-a4571e64d255.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "먼저 output 쪽의 gradient는 $ \\frac{\\partial f}{\\partial f}$로 1이다.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52547436-0c74a600-2e0b-11e9-9dbd-52d68d38afb3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "q * z = f 이기 때문에  $\\frac{\\partial f}{\\partial z}$ = q 즉 3 이다. 반대로 $\\frac{\\partial f}{\\partial q}$ = z 즉 -4가 된다.\n",
    "\n",
    " 또한 q = x+y 라서 $\\frac{\\partial q}{\\partial x} = 1, \\frac{\\partial f}{\\partial y} = 1$ 이다.\n",
    "\n",
    "여기까지는 Chain Rule이 적용되지 않았다.\n",
    "\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52547489-7a20d200-2e0b-11e9-98d0-315e2fc49441.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "f를 x로 미분한 값은 Chain_Rule을 통해 f의 q에 대한 미분과 q의 x에대한 미분을 곱한 값이다($\\frac{\\partial f}{\\partial y} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial y}$) \n",
    "\n",
    "따라서 아까 구해놨던 $\\frac{\\partial f}{\\partial q} = -4, \\frac{\\partial q}{\\partial y} = 1$ 를 대입하면 \n",
    "\n",
    "## $\\frac{\\partial f}{\\partial y} = -4$이다.(마찬가지로 $\\frac{\\partial f}{\\partial x} = -4$이다)\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52039631-67d7a600-2578-11e9-80bc-a5d8952b5551.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "class를 만들어서 전체 과정을 구현해보았다. 곱셈 Layer과 덧셈 Layer을 만들었다. (input이 벡터가 아니라 스칼라)\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈계층\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x \n",
    "        \n",
    "        return dx, dy\n",
    "    \n",
    "# 덧셈계층\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout): \n",
    "        dx = dout*1\n",
    "        dy = dout*1\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:  -12\n",
      "dq: -4\n",
      "dz: 3\n",
      "dx: -4\n",
      "dy: -4\n"
     ]
    }
   ],
   "source": [
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "\n",
    "add_layer = AddLayer()\n",
    "mul_layer = MulLayer()\n",
    "\n",
    "# 순전파\n",
    "q = add_layer.forward(x,y)\n",
    "f = mul_layer.forward(q,z)\n",
    "print(\"f: \" ,f)\n",
    "\n",
    "#역전파\n",
    "df = 1\n",
    "dq, dz = mul_layer.backward(1)\n",
    "print(\"dq:\", dq)\n",
    "print(\"dz:\", dz)\n",
    "\n",
    "dx, dy = add_layer.backward(dq)\n",
    "print(\"dx:\", dx)\n",
    "print(\"dy:\", dy) # dx, dy는 -4 나온것 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52548184-8149df00-2e0f-11e9-9bf4-23efe21500cf.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "전체 과정을 도식화한 내용입니다. \n",
    "\n",
    "gradient가 전달되면(Upstran gradient) 연결된 바로 직전의 node에 gradient를 전달(Local gradient)하게 됩니다. \n",
    "\n",
    "더 복잡한 예제도 결국 Chain Rule을 통해서 구할 수 있습니다.다른 예제도 풀어보자!\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52042800-62cb2480-2581-11e9-93e5-0cf6d5a5adc3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 추가 계층 생성\n",
    "\n",
    "## 역수계층\n",
    "class  InverseLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = 1/x\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, x):\n",
    "        self.x = x\n",
    "        dx = dout * -1 / (self.x) ** 2\n",
    "        return dx\n",
    "        \n",
    "        \n",
    "# 지수계층\n",
    "class ExpLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.exp(x)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, x):\n",
    "        dx = np.exp(x) * dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: -2\n",
      "q: 6\n",
      "r: 4\n",
      "s: 1\n",
      "t: -1\n",
      "u: 0.36787944117144233\n",
      "v: 1.3678794411714423\n",
      "L: 0.7310585786300049\n"
     ]
    }
   ],
   "source": [
    "w0 = 2\n",
    "x0 = -1\n",
    "w1 = -3\n",
    "x1 = -2\n",
    "w2 = -3\n",
    "\n",
    "add_layer1 = AddLayer()\n",
    "add_layer2 = AddLayer()\n",
    "add_layer3 = AddLayer()\n",
    "mul_layer1 = MulLayer()\n",
    "mul_layer2 = MulLayer()\n",
    "mul_layer3 = MulLayer()\n",
    "inv_layer = InverseLayer()\n",
    "exp_layer = ExpLayer()\n",
    "\n",
    "# 순전파\n",
    "p = mul_layer1.forward(w0,x0)\n",
    "q = mul_layer2.forward(w1,x1)\n",
    "r = add_layer1.forward(p,q)\n",
    "s = add_layer2.forward(r,w2)\n",
    "t = mul_layer3.forward(s,-1)\n",
    "u = exp_layer.forward(t)\n",
    "v = add_layer3.forward(u,1)\n",
    "L = inv_layer.forward(v)\n",
    "\n",
    "print(\"p:\", p)\n",
    "print(\"q:\", q)\n",
    "print(\"r:\", r)\n",
    "print(\"s:\", s)\n",
    "print(\"t:\", t)\n",
    "print(\"u:\", u)\n",
    "print(\"v:\", v)\n",
    "print(\"L:\", L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz: 1\n",
      "dv: -0.534446645388523\n",
      "du: -0.534446645388523\n",
      "dt: -0.19661193324148188\n",
      "ds: 0.19661193324148188\n",
      "dr: 0.19661193324148188\n",
      "dp: 0.19661193324148188\n",
      "dq: 0.19661193324148188\n",
      "dw0: -0.39322386648296376\n",
      "dx0: -0.5898357997244457\n",
      "dw1: -0.19661193324148188\n",
      "dx1: 0.39322386648296376\n",
      "dw2: 0.19661193324148188\n"
     ]
    }
   ],
   "source": [
    "#역전파\n",
    "dz = 1\n",
    "dv = inv_layer.backward(dz,v)\n",
    "du, _ = add_layer3.backward(dv) # 1에대한 미분값은 필요없으므로\n",
    "dt  = exp_layer.backward(du,t)\n",
    "ds,_ = mul_layer3.backward(dt) # 1에 대한 미분값은 필요없으므로\n",
    "dr, dw2 = add_layer2.backward(ds)\n",
    "dp, dq = add_layer1.backward(dr)\n",
    "dw0, dx0 = mul_layer2.backward(dp)\n",
    "dw1, dx1 = mul_layer1.backward(dq)\n",
    "\n",
    "\n",
    "print('dz:',dz)\n",
    "print('dv:',dv)\n",
    "print('du:',du)\n",
    "print('dt:',dt)\n",
    "print('ds:',ds)\n",
    "print('dr:',dr)\n",
    "print('dp:',dp)\n",
    "print('dq:',dq)\n",
    "print('dw0:',dw0)\n",
    "print('dx0:',dx0)\n",
    "print('dw1:',dw1)\n",
    "print('dx1:',dx1)\n",
    "print('dw2:',dw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52551453-ef969d80-2e1f-11e9-8977-657f14dbdf35.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "값들이 일치하는 것을 확인할 수 있다. 초록색이 순전파값, 빨간색이 역전파값\n",
    "\n",
    "gradient의 장점은 어떠한 복잡한 node도 group화 시켜서 bignode로 바꿀 수 있다는 점입니다.\n",
    "\n",
    "예를 들어 sigmod function의 경우 위처럼 전 과정을 할 수 도 있지만 sigmoid gate를 묶어서도 가능\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52548814-1bf7ed00-2e13-11e9-9cbb-d28c3e3bad11.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지수계층\n",
    "class Sigmoid_Gate_Layer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = 1/(1+np.exp(-1*x))\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout, x):\n",
    "        out = 1/(1+np.exp(-1*x))\n",
    "        dx = out * (1-out) * dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7755575615628914e-17\n"
     ]
    }
   ],
   "source": [
    "sigmoid_layer = Sigmoid_Gate_Layer()\n",
    "dz = 1\n",
    "ds2 = sigmoid_layer.backward(dz,s) # s가 input\n",
    "print(ds - ds2) # 차이가 0에 가깝다. 같은값 도출된것 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52549539-05ec2b80-2e17-11e9-955a-02cb406044b7.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "add gate의 경우 현재의 gradient를 각각의 노드에 분배. \n",
    "\n",
    "mul gate는 현재의 gradient를 각각 숫자에 곱해서 바꿔치기한다. \n",
    "\n",
    "max gate는 더 큰쪽에만 gradient를 그대로 꽂아주고 반대쪽은 0을 준다.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52551658-caeef580-2e20-11e9-8921-52bb867d5470.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "- 벡터로 확장\n",
    "\n",
    "벡터로 확장하면 Jacobian matrix가 필요.  $\\frac{\\partial z}{\\partial x}$ 는 Jacobian matrix이다. \n",
    "\n",
    "참고자료 :[동훈님의 공돌이의 수학정리노트](https://wikidocs.net/4053), [다크프로그래머님의 블로그](https://darkpgmr.tistory.com/132?category=460967)\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/05e6f6d900ffe45c1eca200bf45fa6724dac81a2/68747470733a2f2f692e696d6775722e636f6d2f4f6d35745a4e6d2e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://camo.githubusercontent.com/b77567058f65de23ed59bd0072a22efaf67ca74a/68747470733a2f2f692e696d6775722e636f6d2f75754f656f6e622e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52556694-a6027e80-2e30-11e9-92bb-bb9352c21cd3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Q1:  위 그림에서 자코비안 매트릭스의 사이즈와 형태는?\n",
    "\n",
    "A1: 4096 X 4096이다. Diagonal.\n",
    "\n",
    "![](https://user-images.githubusercontent.com/36406676/52556706-adc22300-2e30-11e9-8500-8a97044fb209.PNG) 그림을 보면 이해 가능\n",
    "\n",
    "&nbsp;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52558435-b36e3780-2e35-11e9-9479-e636a0663977.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://user-images.githubusercontent.com/36406676/52558445-bb2ddc00-2e35-11e9-8e22-01844e810201.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 이해가 안되는구만.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
